{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import webbrowser\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "# 網頁 URL\n",
    "url = \"https://www.mobile01.com/marketcommoditylist.php?c=16&s=20\"\n",
    "\n",
    "# 使用 cloudscraper 來繞過 Cloudflare 的挑戰\n",
    "headers = {\n",
    "\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\"\n",
    "}\n",
    "scraper = cloudscraper.create_scraper()\n",
    "res = scraper.get(url, headers=headers).text\n",
    "# 使用 BeautifulSoup 解析 HTML\n",
    "soup = bs(res, \"lxml\")\n",
    "# 找到所有分頁按鈕並提取最後一個頁碼\n",
    "pagination_elements = soup.select('li.l-pagination__page > a.c-pagination[data-page]')\n",
    "if not pagination_elements:\n",
    "    raise ValueError(\"未找到任何分頁按鈕，請檢查選擇器是否正確。\")\n",
    "\n",
    "# 提取最後一個分頁的 data-page 值\n",
    "last_page_number = int(pagination_elements[-1][\"data-page\"])\n",
    "print(\"最後一個頁碼是:\", last_page_number)\n",
    "\n",
    "def new_page(page_url):\n",
    "    # 使用正確的選擇器來提取商品名稱和連結\n",
    "    scraper_page_url = cloudscraper.create_scraper()\n",
    "    res = scraper_page_url.get(page_url, headers=headers).text\n",
    "    soup = bs(res, \"lxml\")\n",
    "    items = soup.select('div > div.u-gapBottom--lg > a.c-articleCard')\n",
    "    # print(items)\n",
    "    for i, item in enumerate(items, 1):\n",
    "\n",
    "        title = item.text.strip()  # 商品名稱\n",
    "        link = f\"https://www.mobile01.com{item['href']}\"  # 完整商品連結\n",
    "        # print(link)\n",
    "        open_web(link)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def open_file(items):\n",
    "    # 檔案名稱\n",
    "    filename = \"M01.json\"\n",
    "\n",
    "    # 如果檔案已存在，載入舊資料\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "            existing_data = json.load(file)  # 載入現有資料\n",
    "    else:\n",
    "        existing_data = []  # 初始化空列表\n",
    "\n",
    "    # 新增資料到舊資料中\n",
    "    existing_data.extend(items)\n",
    "\n",
    "    # 寫入到 JSON 檔案\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(existing_data, file, ensure_ascii=False, indent=4)  # Pretty print JSON\n",
    "        # print(f\"資料已成功儲存到 {filename}\")\n",
    "\n",
    "\n",
    "\n",
    "def open_web(link):\n",
    "    #print(f\"正在打開 {link} 的網頁...\")\n",
    "    # 打开目标网页\n",
    "    url = link\n",
    "    # webbrowser.open(link)  # 在預設瀏覽器中打開連結\n",
    "    scraper_link = cloudscraper.create_scraper()\n",
    "    res = scraper_link.get(link, headers=headers).text\n",
    "    soup = bs(res, \"lxml\")\n",
    "    elements_title = soup.select_one('div.t2.u-vam > span.u-vam')    \n",
    "    elements_releasedate = soup.select_one('div.l-input__item.l-input__item--min > span.o-fNotes')\n",
    "    elements_price = soup.select_one('td.c-detail__info.c-detail__info--middle > span.t2.o-fHighlight')\n",
    "    elements_content = soup.select_one('div.l-tab > div.l-tab__container.is-active')\n",
    "    #再寫一欄網址的\n",
    "    # 輸出結果 \n",
    "    # 提取每个元素的文本内容并放入列表\n",
    "    \n",
    "    title = [element.text.strip() for element in elements_title]\n",
    "    releasedate = [element.text.strip() for element in elements_releasedate]\n",
    "    price = [element.text.strip() for element in elements_price]\n",
    "    content = [element.text.strip() for element in elements_content]\n",
    "    # 使用正則表達式清理空白行\n",
    "    cleaned_content = \"\\n\".join(line.strip() for line in content if line.strip())\n",
    "\n",
    "    # 確保 cleaned_content 是一個列表，且每段內容對應到相應的項目\n",
    "    cleaned_content_list = cleaned_content.split(\"\\n\\n\")  # 用雙換行符號分段，形成列表\n",
    "\n",
    "    # 確認 cleaned_content_list 與其他列表的長度一致\n",
    "    if len(cleaned_content_list) != len(title):\n",
    "        raise ValueError(\"cleaned_content_list 和其他欄位的長度不一致！請檢查資料來源。\")\n",
    "    \n",
    "    # 使用 zip 將各欄位合併為字典\n",
    "    items = [\n",
    "        {\"title\": t, \"releasedate\": r, \"price\": p, \"content\": c, \"link\": l} \n",
    "        for t, r, p, c, l in zip(title, releasedate, price, cleaned_content_list, [url])\n",
    "    ]\n",
    "    open_file(items)\n",
    "    # # 顯示結果\n",
    "    # for item in items:\n",
    "    #     print(f\"Title: {item['title']}, Releasedate: {item['releasedate']}, Price: {item['price']}, Content: {item['content']}\")\n",
    "    \n",
    " \n",
    "for i in range (1, last_page_number + 1):\n",
    "    # params[\"p\"] = i\n",
    "    page_url = f\"https://www.mobile01.com/marketcommoditylist.php?c=16&s=20&p={i}\"\n",
    "    new_page(page_url)\n",
    "    print(page_url)   \n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
